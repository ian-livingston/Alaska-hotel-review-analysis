{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Set-up\n",
    "I built my project around data scraped from Tripadvisor and specifically from listings of (~) every Alaska property listed on the site. Below, I've pieced together my workflow as best I can, and hopefully in a way that can be reasonably understood by anyone reading it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time, os\n",
    "from fake_useragent import UserAgent\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from Functions as akf\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "nltk.download('stopwords')\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import spacy\n",
    "from collections import Counter\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "ua = UserAgent()\n",
    "user_agent = {'User-agent': ua.random}\n",
    "chromedriver = \"/Applications/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "source": [
    "# Building main dataset\n",
    "I first collected URLs of all and then passed the list of scraped URLs to the main review-scraping function to collect my data. In navigating Selenium hiccups I skipped the following indices in my URL list: 138, 165, 188, 228, 324, 348, 349, 350, 351, 352, 431, 453, and 488."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting URLs\n",
    "akf.get_reviews_hotel_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hotel_urls.pickle\", \"rb\") as to_read:\n",
    "    url_list = pickle.load(to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting reviews\n",
    "get_reviews(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alaska_hotels_df.pickle\", \"rb\") as to_read:\n",
    "    main_df = pickle.load(to_read)"
   ]
  },
  {
   "source": [
    "# EDA\n",
    "The below illustrates a few of the many steps I took in examining for the first time the data I'd scraped as a whole."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the rows that for whatever reason do not have full reviews\n",
    "main_df = main_df[main_df['Full review'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the number of words in the corpus at this point...\n",
    "X = main_df[\"Full review\"]\n",
    "all_strings = \" \".join(X.values)\n",
    "splits = all_strings.split()\n",
    "print(f'Number of strings: {len(splits)}')\n",
    "print(f'Number of unique strings: {len(set(splits))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and at the top-appearing words (to be repeated many times after further pre-processing)...\n",
    "freq_splits = FreqDist(splits)\n",
    "print(f'25 most common strings: \\n{freq_splits.most_common(25)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and at the top-appearing words < 4 characters (to be repeated many times after further pre-processing)...\n",
    "short = set(s for s in splits if len(s) < 4)\n",
    "short = [(s, freq_splits[s]) for s in short]\n",
    "short.sort(key=lambda x:x[1], reverse=True)\n",
    "print(f'25 most common short strings:{short[:25]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and at the top-appearing words > 15 characters (to be repeated many times after further pre-processing)...\n",
    "long = set(s for s in splits if len(s) > 15)\n",
    "long = [(s, freq_splits[s]) for s in long]\n",
    "long.sort(key=lambda x:x[1], reverse=True)\n",
    "print(f'25 most common short strings:{long[:25]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying all digits in the corpus\n",
    "akf.summarize(r\"\\d\", splits, freq_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying all words with - in the corpus\n",
    "akf.summarize(r\"\\w+-+\\w+\", splits, freq_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying all words with extra intentional letters\n",
    "outlaws = [s for s in splits if akf.find_outlaw(s)]\n",
    "print(\"{} strings which is {:.2%} of total\".format(len(outlaws), len(outlaws) / len(splits)))\n",
    "outlaw_freq = [(s, freq_splits[s]) for s in set(outlaws)]\n",
    "outlaw_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "print(outlaw_freq)"
   ]
  },
  {
   "source": [
    "# Cleaning/pre-processing\n",
    "Below are the primary means by which I cleaned up my corpus ahead of topic modeling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell-checking and -replacing\n",
    "main_df = akf.spell_checker(all_hotels_df, pickling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning (using Regex) and lemmatizing review\n",
    "main_df = akf.review_cleaner(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning reviews further\n",
    "main_df[\"Cleaned review v2\"] = main_df[\"Cleaned review\"].apply(lambda x: akf.clean_again(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning reviews even further\n",
    "main_df[\"Cleaned review v3\"] = main_df[\"Cleaned review v2\"].apply(lambda x: akf.third_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning city names from reviews\n",
    "cities = []\n",
    "for address in tqdm(list(main_df[\"Property address\"].unique())):\n",
    "    try:\n",
    "        cities.append(address.split(', ')[1])\n",
    "    except AttributeError:\n",
    "        continue\n",
    "cities = [city for city in cities if cities.count(city) > 15]\n",
    "cities = list(set(cities))\n",
    "cities = [city.lower() for city in cities]\n",
    "cities.remove(\"king salmon\")\n",
    "main_df[\"Clean review no cities\"] = main_df[\"Cleaned review v3\"].apply(lambda x: akf.city_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding a column of reviews as only nouns\n",
    "review_nouns_list = []\n",
    "for i in range(main_df.shape[0]):\n",
    "    review_nouns_list.append(akf.get_nouns(main_df.iloc[i][\"Clean review no cities\"]))\n",
    "full_df[\"Review nouns\"] = review_nouns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column of reviews as only adjectives\n",
    "review_adjs_list = []\n",
    "for i in range(main_df.shape[0]):\n",
    "    review_adjs_list.append(akf.get_adjs(main_df.iloc[i][\"Clean review no cities\"]))\n",
    "full_df[\"Review adjs\"] = review_adjs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding a \"Review length\" column\n",
    "main_df[\"Review length\"] = main_df[\"Full review\"].apply(lambda x: akf.review_length(x))"
   ]
  },
  {
   "source": [
    "# Adding metadata, etc., to main_df\n",
    "At a number of points in my process I added metadata to and cleaned metadata in my main dataset. Among them:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding columns of type datetime for year and month\n",
    "main_df[\"Month of stay\"] = pd.DatetimeIndex(main_df[\"Date of stay\"]).month\n",
    "main_df[\"Year of stay\"] = pd.DatetimeIndex(main_df[\"Date of stay\"]).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting \"Lat, long\" and adding \"Latitude\" and \"Longitude\" coumns with float values\n",
    "for_mapping_df = main_df[main_df[\"Lat, long\"].notna()]\n",
    "lats = []\n",
    "longs = []\n",
    "for i in range(for_mapping_df.shape[0]):\n",
    "    lats.append(float(for_mapping_df.iloc[i][\"Lat, long\"].split(\",\")[0]))\n",
    "    longs.append(float(for_mapping_df.iloc[i][\"Lat, long\"].split(\",\")[1]))\n",
    "for_mapping_df[\"Latitude\"] = lats\n",
    "for_mapping_df[\"Longitude\"] = longs"
   ]
  },
  {
   "source": [
    "# Pickling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alaska_hotels_df.pickle\", \"wb\") as to_write:\n",
    "    pickle.dump(main_df, to_write)"
   ]
  }
 ]
}