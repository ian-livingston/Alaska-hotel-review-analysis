{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Set-up\n",
    "In this notebook I've provided the primary code I used to perform topic modeling and Vader setinment analyis. More of my code and thought can be seen in the Functions.py file in this repo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from corextopic import corextopic as ct\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize, FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import time, os\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from Functions as akf\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pickle\n",
    "from corextopic import corextopic as ct\n",
    "import spacy\n",
    "from collections import Counter\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "source": [
    "# Unpickling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alaska_hotels_df.pickle\", \"rb\") as to_read:\n",
    "    main_df = pickle.load(to_read)"
   ]
  },
  {
   "source": [
    "# Running Vader sentiment analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Vader on reviews in the full df\n",
    "main_df = akf.vader_scores(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column reflecting net sentiment for each review\n",
    "main_df[\"Sentiment net\"] = main_df[\"Vader +\"] - main_df[\"Vader -\"]"
   ]
  },
  {
   "source": [
    "# Trying pyLDAvis\n",
    "This didn't yield much but it did inform the parts of my process that followed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = 'english', lowercase = True, max_df = 0.5)\n",
    "nl_df_vectorized = vectorizer.fit_transform(nl_X.values)\n",
    "nl_lda = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "nl_lda.fit(nl_df_vectorized)\n",
    "pyLDAvis.sklearn.prepare(nl_lda, nl_df_vectorized, vectorizer)"
   ]
  },
  {
   "source": [
    "# Topic modeling with NMF and LSA\n",
    "My attempts with NMF and LSA, as well as LDA above, didn't yield clean topics, but I've included mention (and an example call) to acknowledge this part of my process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of NMF topic modeling with example parameters\n",
    "akf.run_model(main_df, \"nmf\", 8, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of LSA topic modeling with example parameters\n",
    "akf.run_model(main_df, \"lsa\", 8, ngram_range=(1,2))"
   ]
  },
  {
   "source": [
    "# Running CorEx\n",
    "After days of fitting and tuning, CorEx ended up as the foundation of my project. Below is a sample of my CorEx modeling. I saved all (~) of the resulting measures/attributes in a df for analysis later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the df in which to store results\n",
    "corex_df = pd.DataFrame(columns=[\"Topics\", \"Anchors\", \"Topic words\", \"TC\", \"TCs\", \"Labels\", \"Clusters\", \"Alpha\", \"Mis\", \"P(y)|x\", \"Words\", \"N-gram range\", \"min_df\", \"max_df\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the corpus\n",
    "corpus = main_df[\"Clean review no cities\"]\n",
    "vectorizer = TfidfVectorizer(max_df=.5,\n",
    "    min_df=5,\n",
    "    max_features=None,\n",
    "    ngram_range=(1, 2),\n",
    "    norm=None,\n",
    "    binary=True,\n",
    "    use_idf=False,\n",
    "    sublinear_tf=False)\n",
    "vectorizer = vectorizer.fit(corpus)\n",
    "tfidf = vectorizer.transform(corpus)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running CorEx with a range of topic parameters and an otherwise fixed set of parameters\n",
    "# I did this with a ton of anchor combos\n",
    "for i in tqdm(range(3, 28)):\n",
    "    topic_model = ct.Corex(n_hidden=i, words=words, seed=10)\n",
    "    topic_model.fit(tfidf, words=words, docs=corpus, anchors=[[\"hotel\", \"lodge\"], [\"aurora\", \"northern lights\"]])\n",
    "\n",
    "    topic_words = []\n",
    "    for x, topic_ngrams in enumerate(topic_model.get_topics(n_words=10)):\n",
    "        topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "        topic_words.append(\"Topic #{}: {}\".format(x+1, \", \".join(topic_ngrams)))\n",
    "\n",
    "    corex_details = {\"Topics\": i, \"Anchors\": [[\"hotel\", \"lodge\"], [\"aurora\", \"northern lights\"]], \"Topic words\": topic_words, \"TC\": topic_model.tc, \"TCs\": topic_model.tcs, \"Labels\": topic_model.labels, \"Clusters\": topic_model.clusters, \"Alpha\": topic_model.alpha, \"Mis\": topic_model.mis, \"P(y)|x\": topic_model.p_y_given_x, \"Words\": topic_model.words, \"N-gram range\": (1, 2), \"min_df\": 5, \"max_df\": 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding P(y|x) and label attributes for best-performing CorEx model to main_df\n",
    "main_df = main_df.merge(pd.DataFrame(corex_df.iloc[8][\"P(y)|x\"]), left_index=True, right_index=True)\n",
    "main_df = main_df.merge(pd.DataFrame(corex_df.iloc[8][\"Labels\"]), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming new columns\n",
    "main_df.rename(columns={\"0_y\": \"Topic 1 label\", \"1_y\": \"Topic 2 label\",\t\"2_y\": \t\"Topic 3 label\", \"3_y\": \"Topic 4 label\", \"4_y\": \"Topic 5 label\", \"5_y\": \"Topic 6 label\", \"6_y\": \"Topic 7 label\", \"7_y\": \"Topic 8 label\", \"8_y\": \"Topic 9 label\", \"9_y\": \"Topic 10 label\", \"10_y\": \"Topic 11 label\"}, inplace=True)\n",
    "main_df.rename(columns={\"0_x\": \"Topic 1 p(y|x)\", \"1_x\": \"Topic 2 p(y|x)\", \"2_x\": \"Topic 3 p(y|x)\", \"3_x\": \"Topic 4 p(y|x)\", \"4_x\": \"Topic 5 p(y|x)\", \"5_x\": \"Topic 6 p(y|x)\", \"6_x\": \"Topic 7 p(y|x)\", \"7_x\": \"Topic 8 p(y|x)\", \"8_x\": \"Topic 9 p(y|x)\", \"9_x\": \"Topic 10 p(y|x)\", \"10_x\": \"Topic 11 p(y|x)\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking topic counts\n",
    "final_model_topic_counts = []\n",
    "for i in range(1, 12):\n",
    "    final_model_topic_counts.append(f'Topic {i}: ' + str(main_df[f'Topic {i} label'].value_counts()[1]))\n",
    "final_model_topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding \"Season of stay\" column to main_df\n",
    "main_df[\"Season of stay\"] = main_df[\"Month of stay\"].apply(lambda x: akf.get_season(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating latitude bins for latitude-based topic analysis\n",
    "lat_long_df[\"Latitude bin\"] = pd.cut(lat_long_df[\"Latitude\"], 5)\n",
    "lat_long_df.groupby(\"Latitude bin\")[\"Topic 5 p(y|x)\"].mean()"
   ]
  },
  {
   "source": [
    "# More analysis..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}